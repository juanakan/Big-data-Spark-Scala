# Big-data-Spark-Scala

## Practica Spark Scala

### 1. creamos el topic.
![Pantallazo de Hive](https://github.com/juanakan/Bigdata-architecture/blob/master/select.PNG)
### 2. creamos el productor en la terminal y mandamos el json.
![Pantallazo de Hive](https://github.com/juanakan/Bigdata-architecture/blob/master/select.PNG)
### 3. creamos el consumidor en scala.
![Pantallazo de Hive](https://github.com/juanakan/Bigdata-architecture/blob/master/select.PNG)
### 4. mostramos el resultado.
![Pantallazo de Hive](https://github.com/juanakan/Bigdata-architecture/blob/master/select.PNG)

1.  Usando la sintaxis vista en clase con el método  Kafka-console-producer.sh 
 
 
Trabajo que debe desarrollar el CONSUMIDOR  Nuestro consumidor deberá también tener dos entornos distintos de trabajo, para poder chequear que Kafka funciona correctamente. 
 
1. Usando la sintaxis de Kafka-console-consumer.sh. En éste caso solo queremos comprobar que nuestro consumidor puede leerlo sin problema  
 
2. Creando un código en Scala donde vamos a ampliar nuestro trabajo. Deberá leerlo igual que en el caso anterior, pero en éste caso  también deberá hacer un tratamiento especial de los datos que vayamos leyendo. Para cada apartado  que se os va a proponer debemos crear funciones de SCALA. 
 
a. Queremos que  filtre (que no aparezcan) del fichero JSON dos palabras que elegiréis cada uno (así evitamos tentaciones de copia jejej). Debo ver el JSON con esas palabras filtradas 
